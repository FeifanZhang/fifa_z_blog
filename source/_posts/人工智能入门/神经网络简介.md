---
title: 神经网络简介
toc: true
mathjax: true
date: 2021-10-17 12:17:42
tags:
- 人工智能
- 神经网络
categories:
- [人工智能, 神经网络]
---
# 神经网络的概念
* 人工神经网络（英文Artificial Neural Network，简写ANN），简称神经网络，是一种模仿动物大脑结构的数学模型，用于对函数进行估计或近似。
* 广泛用于机器视觉、语音识别等传统规则编程解决的问题
# 神经元简介
## 概念
* 在生物学中，大脑由数量极大的神经元组成，神经元之间相互连接，某个神经元“兴奋”时，会向相连的神经元释放化学物质，当相连神经元接受到的化学物质超过“阈值”时，自身亦会“兴奋”，向其他神经元释放化学物质。
* 将生物学中神经元抽象化，得到**M-P神经元模型**（即沿用至今的神经网络模型）
* 神经元是神经网络最小单位
* 神经元之间相互连接
## 神经元模型结构
  ```mermaid
  flowchart LR
  id1((a1)) -->|w1| idsum((SUM))
  id2((a2)) -->|w2| idsum((SUM))
  idn((an)) -->|wn| idsum((SUM))
  idNo((1)) -->|b| idsum((SUM))
  idsum --> f --> idt((t))
  ```
* $a_1$ ... $a_n$为输入的分量
* $w_1$ ... $w_n$为分量对应的权重参数
* $b$为偏置，是一个常数
* $f$是激活函数，常见的激活函数有`tanh、sigmoid、relu`
* 公式表现为：$t = f(W^T A + b)$ （$W^T$是转置）
# 单层神经网络
* 最基础的神经网络形式，神经元数量有限
* 神经元的输入向量是同一个向量，每一个神经元都会产生一个标量结果，所以输出的向量维数等于神经元数量
## 单层神经网络结构
  ```mermaid
  flowchart LR
  i1((输入向量1)) --> n1((神经元1))
  i1 --> n2((神经元2))
  i1 --> n3((神经元3))
  i2((输入向量2)) --> n1
  i2 --> n2
  i2 --> n3
  i3((输入向量3)) --> n1
  i3 --> n2
  i3 --> n3
  n1 --> scal1((标量1))
  n2 --> scal2((标量2))
  n3 --> scal3((标量3))
  ```
# 感知机
* 由两层神经网络组成：输入层和输出层
* 若$ \sum_{n=0}^N (input_n* W_n)$超过其阈值则输出1，反之则输出0
* 其中输入向量称为**输入层**、神经元称为**输出层**
## 感知机的意义
* 将n维空间通过超平面进行分割为两部分，并判断该向量位于超平面的哪一边
* 说人话就是感知机可接受一个n维向量的输入，却只能输出`0或1`，故通过感知机可判断一个向量是`0类`还是`1类`
# 多层神经网络
* 多层神经网络由单层网络进行叠加得到，常见结构如下：
  * **输入层：** 大量神经元接受信息，输入的信息称为输入向量，其神经元数量与向量维度（即信息的特征数量）大致相等
  * **隐藏层：** 输入层与输出层之间的神经元连接组合而成，隐藏层的层数不定，每层神经元数量不一，但数量越多则网络的非线性特征越显著，健壮性越强。
  * **输出层：** 消息在神经元中传输、分析、归类、权衡后进行输出，输出的结果称为输出向量，其神经元数量与希望分类的数量大致相等
* 其中每一层都是一个**单层神经网络**
## 多层神经网络结构
```mermaid
flowchart LR
输入层 -->|经过输入层神经元处理后| 隐藏层1 --> 隐藏层2 -->|...|隐藏层n --> 输出层 
```
## 全连接层
* 当前一层与之前一层的每个神经元都有链接，称其为全连接层
* 若第`N`层有`n`个神经元，`N-1`层有`m`个神经元，当第`N`层为全连接层时，N-1和N层之间有`n*m`个权重参数
* 当第`N`层为全连接层时，N-1到N层的信息传输遵循如下规定：
  * $a_0^{(N-1)}$表示第`N-1`层的第`0`个神经元的输出，$w_{0.1}$表示`第N层的第0个神经元`接受`前一层第1个神经元`信号的权重。当第`N`层为全连接层时，第`0`个神经元的结果表示为：$a_0^{(N)} = \sigma (w_{0.0}a_0^{(N)} + w_{0.1}a_1^{(N)} +...+ w_{0.m}a_m^{(N)} + b_0)$
  * $w_{0.0}a_0^{(N)} + w_{0.1}a_1^{(N)} +...+ w_{0.m}a_m^{(N)}$还可以通过矩阵表示
  $\left[
    \begin{matrix}
      w_{0.0} & w_{0.1}&\cdots&w_{0.m}\\
    \end{matrix}
  \right] *
  \left[
  \begin{matrix}
      a_0^{(N)}\\
      a_1^{(N)}\\
      \vdots \\
      a_m^{(N)}\\
  \end{matrix}
  \right]
  $
  * 以上仅是第N层一个神经元的输入，若考虑N-1层与N层所有神经元的关系则可以用矩阵表示：
  $
  \left[
    \begin{matrix}
        w_{0.0} & w_{0.1}&\cdots&w_{0.m}\\
        w_{1.0} & w_{1.1} &...&w_{1.m}\\
        \vdots & \vdots & \ddots & \vdots\\
        w_{n.0} & w_{n.1} & ...& w_{n.m}\\
      \end{matrix}
    \right]  *
  \left[
    \begin{matrix}
        a_0^{(N)}\\
        a_1^{(N)}\\
        \vdots \\
        a_m^{(N)}\\
    \end{matrix}
  \right]
  $
* 全连接层就是在前一层基础上进行了一次 $y = wx + b$的变化（在不考虑激活函数时就是一次线性变化），线性变化就是偏置`b`与权重`w`的组合
# 激活函数
## 线性函数：
  * 当$f(x) = y$满足如下条件则为线性函数
    * $f(x_1 + x_2) = y_1 + y_2$
    * $f(k x_1) = k y_1$
  * 若神经网络中仅有`权重（w）`以及`偏置（b）`，通过感知机进行的分类永远是线性的（即分类的超平面永远是**直的**） ，而真实情况下，输出结果的分类可能是非线性的（即分类的超平面是**弯曲的**），这也是 **激活函数的意义：增加模型的非线性分割能力**
## 常见的激活函数：
  * Sigmoid函数：
    * 公式：$\sigma(x) = \frac{1}{1+e^{-x}}$
    * 特点：只输出正数，靠近0的输出变化最大
  * tanh函数：$tanh(x)$
  * Maxout函数：$max(w_1^Tx + b_1, w_2^T+b_2)$
  * leakey ReLU函数：
    * 公式：$max(0.1x, x)$
    * 特点：x趋近于负无穷时，y会到负无穷
  * ReLU函数：
    * 公式：$max(0, x)$
    * 特点：x小于0时，y恒等于0，故输入参数为图像，则适合ReLU，因为图像像素取值为[0, 255]
  * ELU函数：
    * 公式：$$
  f(x) =
  \begin{cases}
    x &if\ x \geq 0\\
    \alpha(e^x-1) &if\ x \lt 0\\
  \end{cases}
  $$
    * 特点：
      * $\alpha$作为自定义参数自行设置
      * 与`leakey ReLU`不同，当x小于0时，y会有一个限制，不会无限趋近于负无穷
## 激活函数优点：
* 增加模型非线性分割能力
* 提高模型鲁棒性（即稳健性）
* 加速模型收敛（说白了就是模型训练更快）
* 缓解梯度消失